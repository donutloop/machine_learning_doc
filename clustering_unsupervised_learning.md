# Clustering

* Clusters or groups : Each of these clusters or groups is a collection of data points into which our clustering technique organizes our data points. 

* Intra -group or intra-cluster : Clusters resulting from clustering can be evaluated using a measure of similarity between data points and other data points in the same resulting cluster. This is called intra-group or intra-cluster evaluation and similarity. 

* Inter -group or inter-cluster : Clusters resulting from clustering can be evaluated using a measure of dissimilarity between data points and other data points in other resulting clusters. This is called inter-group or inter-cluster evaluation and dissimilarity. 

* Internal criteria : Often, we do not have a gold standard set of cluster labels that we can use to evaluate our resulting clusters. In these cases, we utilize inter and intra cluster similarities to measure the performance of our clustering technique. 

* External criteria : In other cases, we might have a gold standard for cluster labels or grouping, such as a standard generated by human judges. These scenarios allow us to evaluate our clustering techniques using the standard, or external, criteria.

* Distance or similarity : This is a measure of how close two data points are. This could be a Euclidean distance in the space of your features or some other measure of closeness.

# Clustering data

In order to find these clusters, we use some kind of similarity measure such as Euclidean distance, to find the subgroups. This similarity measure can estimate the tightness of a cluster. We can say that clustering is the process of organizing our data into subgroups whose elements are similar to each other. 

Our goal is to identify the intrinsic properties of data points that make them belong to the same subgroup. There is no universal similarity metric that works for all the cases. It depends on the problem at hand.

For example, we might be interested in finding the representative data point for each subgroup or we might be interested in finding the outliers in our data. Depending on the situation, we will end up choosing the appropriate metric. 


## K-Means algorithm

K-Means algorithm is a well-known algorithm for clustering data. In order to use this algorithm, we need to assume that the number of clusters is known beforehand. We then segment data into K subgroups using various data attributes. We start by fixing the number of clusters and classify our data based on that. The central idea here is that we need to update the locations of these K centroids with each iteration. We continue iterating until we have placed the centroids at their optimal locations. We can see that the initial placement of centroids plays an important role in the algorithm. These centroids should be placed in a clever manner, because this directly impacts the results. A good strategy is to place them as far away from each other as possible. The basic K-Means algorithm places these centroids randomly where K-Means++ chooses these points algorithmically from the input list of data points. It tries to place the initial centroids far from each other so that it converges quickly. We then go through our training dataset and assign each data point to the closest centroid. Once we go through the entire dataset, we say that the first iteration is over. We have grouped the points based on the initialized centroids. We arrived at the final locations of the centroids. These K centroids are the final K Means that will be used for inference. Let's apply K-Means clustering on two-dimensional data to see how it works. We will be using the data in the data_clustering.txt file provided to you. Each line contains two comma-separated numbers.

## k-nearest neighbors

### Datapoint distance or similarity

* Minkowski distance 
* cosine similarity 
* Jaccard Similarity
* Euclidean distance